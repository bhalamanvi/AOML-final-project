{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99d4ce37",
        "outputId": "b0b285a0-2b6e-40e4-f3ba-a2753e162dec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "Performing feature engineering...\n",
            "Preprocessing complete. Data saved to preprocessed_recipes.csv\n",
            "Loading preprocessed data...\n",
            "Training models...\n",
            "Training model for ProteinContent...\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Early stopping, best iteration is:\n",
            "[4]\tvalid_0's rmse: 0.00115498\n",
            "ProteinContent model evaluation:\n",
            "  RMSE: 0.00115, MAE: 0.00004, R² Score: 0.00281\n",
            "ProteinContent model evaluation:\n",
            "  accuracy: 1.0000\n",
            "  precision: 1.0000\n",
            "  recall: 1.0000\n",
            "  f1: 1.0000\n",
            "Training model for CarbohydrateContent...\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Early stopping, best iteration is:\n",
            "[15]\tvalid_0's rmse: 0.00251646\n",
            "CarbohydrateContent model evaluation:\n",
            "  RMSE: 0.00252, MAE: 0.00003, R² Score: 0.00489\n",
            "CarbohydrateContent model evaluation:\n",
            "  accuracy: 0.9999\n",
            "  precision: 1.0000\n",
            "  recall: 0.9999\n",
            "  f1: 0.9999\n",
            "Training model for FiberContent...\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Early stopping, best iteration is:\n",
            "[9]\tvalid_0's rmse: 0.000934928\n",
            "FiberContent model evaluation:\n",
            "  RMSE: 0.00093, MAE: 0.00007, R² Score: 0.09020\n",
            "FiberContent model evaluation:\n",
            "  accuracy: 1.0000\n",
            "  precision: 0.9999\n",
            "  recall: 1.0000\n",
            "  f1: 0.9999\n",
            "Training model for FatContent...\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Early stopping, best iteration is:\n",
            "[12]\tvalid_0's rmse: 0.00125858\n",
            "FatContent model evaluation:\n",
            "  RMSE: 0.00126, MAE: 0.00003, R² Score: 0.15404\n",
            "FatContent model evaluation:\n",
            "  accuracy: 0.9998\n",
            "  precision: 0.9998\n",
            "  recall: 0.9998\n",
            "  f1: 0.9998\n",
            "Model training complete. Models saved to disk.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import csv\n",
        "import numpy as np\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "import re\n",
        "import pickle\n",
        "import os\n",
        "from multiprocessing import Pool, cpu_count\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "def load_and_preprocess_data(data_path):\n",
        "    \"\"\"\n",
        "    Load and preprocess the recipe dataset with optimizations\n",
        "    \"\"\"\n",
        "    print(\"Loading data...\")\n",
        "    # Use low_memory=False and only parse necessary columns to speed up loading\n",
        "    column_dtypes = {\n",
        "        'RecipeId': 'str',\n",
        "        'Name': 'str',\n",
        "        'AuthorName': 'str',\n",
        "        'Description': 'str',\n",
        "        'Images': 'str',\n",
        "        'RecipeYield': 'str',\n",
        "        'RecipeInstructions': 'str',\n",
        "    }\n",
        "\n",
        "    # First, read just the columns we need for display to save memory\n",
        "    display_columns = ['RecipeId', 'Name', 'AuthorName', 'Description', 'Images',\n",
        "                      'RecipeYield', 'RecipeInstructions']\n",
        "\n",
        "    display_df = pd.read_csv(data_path, usecols=display_columns, dtype=column_dtypes,\n",
        "                            low_memory=False)\n",
        "\n",
        "    # Save the display dataframe for later use\n",
        "    pickle.dump(display_df, open('display_data.pkl', 'wb'))\n",
        "    del display_df  # Free up memory\n",
        "\n",
        "    # Now read only the model columns\n",
        "    model_columns = ['RecipeId', 'CookTime', 'PrepTime', 'TotalTime', 'DatePublished',\n",
        "                    'RecipeCategory', 'Keywords', 'RecipeIngredientQuantities',\n",
        "                    'RecipeIngredientParts', 'AggregatedRating', 'ReviewCount',\n",
        "                    'Calories', 'FatContent', 'SaturatedFatContent', 'CholesterolContent',\n",
        "                    'SodiumContent', 'CarbohydrateContent', 'FiberContent',\n",
        "                    'SugarContent', 'ProteinContent', 'RecipeServings']\n",
        "\n",
        "    # Use chunks to process large files more efficiently\n",
        "    chunk_size = 10000\n",
        "    chunks = []\n",
        "\n",
        "    for chunk in pd.read_csv(data_path, usecols=model_columns, chunksize=chunk_size, low_memory=False, on_bad_lines=\"skip\", quoting=csv.QUOTE_NONE):\n",
        "        # Process each chunk\n",
        "        chunk = preprocess_chunk(chunk)\n",
        "        chunks.append(chunk)\n",
        "\n",
        "    # Combine chunks\n",
        "    df = pd.concat(chunks, ignore_index=True)\n",
        "\n",
        "    # Feature engineering - only do this once on the combined data\n",
        "    df = feature_engineering(df)\n",
        "\n",
        "    return df\n",
        "\n",
        "def preprocess_chunk(df):\n",
        "    \"\"\"\n",
        "    Process a chunk of the dataframe\n",
        "    \"\"\"\n",
        "    # Convert time columns efficiently\n",
        "    for col in ['CookTime', 'PrepTime', 'TotalTime']:\n",
        "        if col in df.columns:\n",
        "            df[col] = df[col].apply(lambda x: convert_time_to_minutes(x) if isinstance(x, str) else x)\n",
        "\n",
        "    # Handle missing values more efficiently using SimpleImputer instead of KNN for speed\n",
        "    df = handle_missing_values(df)\n",
        "\n",
        "    return df\n",
        "\n",
        "def handle_missing_values(df):\n",
        "    \"\"\"\n",
        "    Handle missing values efficiently\n",
        "    \"\"\"\n",
        "    # Numerical columns to be imputed\n",
        "    num_cols = ['CookTime', 'PrepTime', 'TotalTime', 'AggregatedRating',\n",
        "                'ReviewCount', 'Calories', 'FatContent', 'SaturatedFatContent',\n",
        "                'CholesterolContent', 'SodiumContent', 'CarbohydrateContent',\n",
        "                'FiberContent', 'SugarContent', 'ProteinContent', 'RecipeServings']\n",
        "\n",
        "    # Check which columns exist in the dataframe\n",
        "    num_cols = [col for col in num_cols if col in df.columns and pd.api.types.is_numeric_dtype(df[col])]\n",
        "\n",
        "    # Use median imputation instead of KNN for speed\n",
        "    if num_cols:\n",
        "        imputer = SimpleImputer(strategy='median')\n",
        "        df_num = df[num_cols].copy()\n",
        "\n",
        "        # Only apply imputation if there are missing values\n",
        "        if df_num.isna().any().any():\n",
        "            df_num_imputed = pd.DataFrame(imputer.fit_transform(df_num), columns=num_cols)\n",
        "            df[num_cols] = df_num_imputed\n",
        "\n",
        "    # Handle categorical columns with faster operations\n",
        "    for col, default in [\n",
        "        ('RecipeCategory', 'Unknown'),\n",
        "        ('Keywords', ''),\n",
        "        ('RecipeIngredientQuantities', ''),\n",
        "        ('RecipeIngredientParts', '')\n",
        "    ]:\n",
        "        if col in df.columns and df[col].isna().any():\n",
        "            df[col] = df[col].fillna(default)\n",
        "\n",
        "    # Handle DatePublished more efficiently\n",
        "    if 'DatePublished' in df.columns and df['DatePublished'].isna().any():\n",
        "        # Use a default date instead of calculating mode for speed\n",
        "        df['DatePublished'] = df['DatePublished'].fillna('2000-01-01')\n",
        "\n",
        "        # Only process if column exists and extract only the year and month\n",
        "        # This is faster than full datetime parsing\n",
        "        try:\n",
        "            df['Year'] = pd.to_datetime(df['DatePublished'], errors='coerce').dt.year\n",
        "            df['Month'] = pd.to_datetime(df['DatePublished'], errors='coerce').dt.month\n",
        "\n",
        "            # Fill NA values with medians\n",
        "            df['Year'] = df['Year'].fillna(df['Year'].median() if not df['Year'].empty else 2000)\n",
        "            df['Month'] = df['Month'].fillna(df['Month'].median() if not df['Month'].empty else 6)\n",
        "        except:\n",
        "            # If datetime conversion fails, use default values\n",
        "            df['Year'] = 2000\n",
        "            df['Month'] = 6\n",
        "\n",
        "    return df\n",
        "\n",
        "def convert_time_to_minutes(time_str):\n",
        "    \"\"\"\n",
        "    Optimized function to convert ISO 8601 duration format to minutes\n",
        "    \"\"\"\n",
        "    if not isinstance(time_str, str):\n",
        "        return np.nan\n",
        "\n",
        "    # Fast pattern matching\n",
        "    total_minutes = 0\n",
        "\n",
        "    # Extract hours (look for pattern like \"1H\")\n",
        "    h_match = re.search(r'(\\d+)H', time_str)\n",
        "    if h_match:\n",
        "        total_minutes += int(h_match.group(1)) * 60\n",
        "\n",
        "    # Extract minutes (look for pattern like \"30M\")\n",
        "    m_match = re.search(r'(\\d+)M', time_str)\n",
        "    if m_match:\n",
        "        total_minutes += int(m_match.group(1))\n",
        "\n",
        "    return total_minutes\n",
        "\n",
        "# Modify your feature_engineering function to preserve the target columns\n",
        "def feature_engineering(df):\n",
        "    \"\"\"\n",
        "    Create new features efficiently\n",
        "    \"\"\"\n",
        "    print(\"Performing feature engineering...\")\n",
        "\n",
        "    # ... rest of your function stays the same ...\n",
        "\n",
        "    # Apply PCA only if we really need to reduce dimensionality\n",
        "    if df.shape[1] > 25:  # Only apply if we have many features\n",
        "        print(\"Applying PCA...\")\n",
        "\n",
        "        # Define target columns that should be preserved\n",
        "        target_cols = ['ProteinContent', 'CarbohydrateContent', 'FiberContent', 'FatContent']\n",
        "\n",
        "        # Get numeric columns that aren't target columns\n",
        "        numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
        "        feature_cols = [col for col in numeric_cols if col not in target_cols and col != 'RecipeId']\n",
        "\n",
        "        if len(feature_cols) > 10:\n",
        "            # Standardize the data\n",
        "            scaler = StandardScaler()\n",
        "            df_numeric = df[feature_cols]\n",
        "            df_numeric_scaled = scaler.fit_transform(df_numeric)\n",
        "\n",
        "            imputer = SimpleImputer(strategy='median')\n",
        "            df_numeric_scaled = imputer.fit_transform(df_numeric_scaled)\n",
        "\n",
        "            # Apply PCA with fewer components for speed\n",
        "            pca = PCA(n_components=min(5, len(feature_cols)))\n",
        "            pca_result = pca.fit_transform(df_numeric_scaled)\n",
        "\n",
        "            # Create PCA dataframe\n",
        "            pca_df = pd.DataFrame(\n",
        "                data=pca_result,\n",
        "                columns=[f'PCA_{i+1}' for i in range(pca_result.shape[1])]\n",
        "            )\n",
        "\n",
        "            # Save the PCA object and scaler for later use\n",
        "            pickle.dump(pca, open('pca_model.pkl', 'wb'))\n",
        "            pickle.dump(scaler, open('scaler_model.pkl', 'wb'))\n",
        "            pickle.dump(feature_cols, open('numeric_cols.pkl', 'wb'))\n",
        "\n",
        "            # Replace original numeric data with PCA components but KEEP target columns\n",
        "            df = df.drop(feature_cols, axis=1)\n",
        "            df = pd.concat([df, pca_df], axis=1)\n",
        "\n",
        "    return df\n",
        "if __name__ == \"__main__\":\n",
        "    data_path = \"recipes.csv\"  # Update with your actual file path\n",
        "    processed_df = load_and_preprocess_data(data_path)\n",
        "\n",
        "    # Save the preprocessed data for model training\n",
        "    processed_df.to_csv(\"preprocessed_recipes.csv\", index=False)\n",
        "    print(f\"Preprocessing complete. Data saved to preprocessed_recipes.csv\")\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
        "import pickle\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def train_recipe_recommendation_model():\n",
        "    \"\"\"\n",
        "    Train a LightGBM model to recommend recipes based on nutritional constraints\n",
        "    with optimizations for speed and performance\n",
        "    \"\"\"\n",
        "    print(\"Loading preprocessed data...\")\n",
        "    # Load data in chunks if it's large\n",
        "    try:\n",
        "        df = pd.read_csv(\"preprocessed_recipes.csv\", low_memory=False)\n",
        "    except:\n",
        "        print(\"Error: Could not find preprocessed data. Please run data preprocessing first.\")\n",
        "        return\n",
        "\n",
        "    # Create nutrition targets for the model (multi-target regression)\n",
        "    targets = ['ProteinContent', 'CarbohydrateContent', 'FiberContent', 'FatContent']\n",
        "\n",
        "    # Ensure all targets are available\n",
        "    for target in targets:\n",
        "        if target not in df.columns:\n",
        "            print(f\"Target column {target} not found in dataset!\")\n",
        "            return\n",
        "\n",
        "    # Keep recipe IDs for later retrieval\n",
        "    recipe_ids = df['RecipeId'].values\n",
        "\n",
        "    # Remove ID and target columns from features\n",
        "    features = df.drop(['RecipeId'] + targets, axis=1)\n",
        "\n",
        "    for col in features.select_dtypes(include=['object']).columns:\n",
        "          le = LabelEncoder()\n",
        "          features[col] = le.fit_transform(features[col].astype(str)) # Convert to string to handle mixed types\n",
        "\n",
        "    # Fill any remaining NaN values with 0 for stability\n",
        "    features = features.fillna(0)\n",
        "    for target in targets:\n",
        "        df[target] = pd.to_numeric(df[target], errors='coerce')\n",
        "\n",
        "    # Fill NaN values in target columns after conversion\n",
        "    df[targets] = df[targets].fillna(df[targets].mean())\n",
        "    # Scale the nutritional targets to improve model performance\n",
        "    scaler = MinMaxScaler()\n",
        "    targets_scaled = scaler.fit_transform(df[targets])\n",
        "\n",
        "    # Save the scaler for prediction\n",
        "    pickle.dump(scaler, open('nutrition_scaler.pkl', 'wb'))\n",
        "\n",
        "    # Train models for each nutritional target\n",
        "    print(\"Training models...\")\n",
        "    models = {}\n",
        "    metrics = {target: {} for target in targets}\n",
        "\n",
        "    # Convert regression to classification for evaluation (simplified binning)\n",
        "    # Create only 5 bins instead of 10 for faster processing\n",
        "    target_bins = {}\n",
        "    for i, target in enumerate(targets):\n",
        "        target_vals = df[target].values\n",
        "        bins = np.linspace(target_vals.min(), target_vals.max(), 6)  # 5 bins instead of 10\n",
        "        target_bins[target] = bins\n",
        "\n",
        "    # Single train/test split instead of full cross-validation for speed\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        features, targets_scaled, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    # Train a separate model for each target\n",
        "    for i, target in enumerate(targets):\n",
        "        print(f\"Training model for {target}...\")\n",
        "\n",
        "        # Get the target values\n",
        "        y_train_target = y_train[:, i]\n",
        "        y_test_target = y_test[:, i]\n",
        "\n",
        "        # Define the model with target-specific parameters\n",
        "        # Use faster training settings\n",
        "        lgb_params = {\n",
        "            'objective': 'regression',\n",
        "            'metric': 'rmse',\n",
        "            'boosting_type': 'gbdt',\n",
        "            'num_leaves': 31,\n",
        "            'learning_rate': 0.1,  # Higher learning rate for faster training\n",
        "            'feature_fraction': 0.8,\n",
        "            'bagging_fraction': 0.8,\n",
        "            'bagging_freq': 5,\n",
        "            'verbose': -1\n",
        "        }\n",
        "\n",
        "        # Create dataset for LightGBM\n",
        "        train_data = lgb.Dataset(X_train, label=y_train_target)\n",
        "        valid_data = lgb.Dataset(X_test, label=y_test_target, reference=train_data)\n",
        "\n",
        "        # Train the model with fewer rounds and early stopping\n",
        "        model = lgb.train(\n",
        "            lgb_params,\n",
        "            train_data,\n",
        "            num_boost_round=500,  # Reduced from 1000\n",
        "            valid_sets=[valid_data],\n",
        "             callbacks=[lgb.early_stopping(stopping_rounds=30),\n",
        "               lgb.log_evaluation(period=100)]\n",
        "        )\n",
        "\n",
        "        # Save the model\n",
        "        models[target] = model\n",
        "\n",
        "        # Make predictions\n",
        "        y_pred = model.predict(X_test)\n",
        "\n",
        "        from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "        rmse = np.sqrt(mean_squared_error(y_test_target, y_pred))\n",
        "        mae = mean_absolute_error(y_test_target, y_pred)\n",
        "        r2 = r2_score(y_test_target, y_pred)\n",
        "\n",
        "        print(f\"{target} model evaluation:\")\n",
        "        print(f\"  RMSE: {rmse:.5f}, MAE: {mae:.5f}, R² Score: {r2:.5f}\")\n",
        "\n",
        "        # For evaluation metrics, convert regression values to classes using bins\n",
        "        y_test_target_reshaped = y_test_target.reshape(-1, 1)\n",
        "        y_test_bins = np.digitize(scaler.inverse_transform(np.repeat(y_test_target_reshaped, 4, axis=1))[:, 0],\n",
        "                                target_bins[target]) - 1\n",
        "\n",
        "        # Similarly, reshape y_pred before inverse_transform\n",
        "        y_pred_reshaped = y_pred.reshape(-1, 1)\n",
        "        y_pred_bins = np.digitize(scaler.inverse_transform(np.repeat(y_pred_reshaped, 4, axis=1))[:, 0],\n",
        "                                target_bins[target]) - 1\n",
        "\n",
        "        # Calculate metrics (using binned values for classification metrics)\n",
        "        metrics[target]['accuracy'] = accuracy_score(y_test_bins, y_pred_bins)\n",
        "        metrics[target]['precision'] = precision_score(y_test_bins, y_pred_bins, average='weighted')\n",
        "        metrics[target]['recall'] = recall_score(y_test_bins, y_pred_bins, average='weighted')\n",
        "        metrics[target]['f1'] = f1_score(y_test_bins, y_pred_bins, average='weighted')\n",
        "\n",
        "        # Print scores\n",
        "        print(f\"{target} model evaluation:\")\n",
        "        for metric, score in metrics[target].items():\n",
        "            print(f\"  {metric}: {score:.4f}\")\n",
        "\n",
        "    # Save the models and recipe IDs\n",
        "    for target, model in models.items():\n",
        "        pickle.dump(model, open(f'model_{target}.pkl', 'wb'))\n",
        "\n",
        "    # Save recipe IDs and target bins\n",
        "    pickle.dump(recipe_ids, open('recipe_ids.pkl', 'wb'))\n",
        "    pickle.dump(target_bins, open('target_bins.pkl', 'wb'))\n",
        "    pickle.dump(targets, open('target_columns.pkl', 'wb'))\n",
        "    pickle.dump(metrics, open('model_metrics.pkl', 'wb'))\n",
        "\n",
        "    print(\"Model training complete. Models saved to disk.\")\n",
        "\n",
        "    # Save feature list for prediction\n",
        "    pickle.dump(features.columns.tolist(), open('feature_columns.pkl', 'wb'))\n",
        "\n",
        "def predict_recipes(protein_constraint, carbs_constraint, fiber_constraint, fat_constraint, limit=10):\n",
        "    \"\"\"\n",
        "    Predict recipes that match the given nutritional constraints\n",
        "    Optimized for speed\n",
        "    \"\"\"\n",
        "    # Load the models and other necessary data\n",
        "    models = {}\n",
        "    targets = pickle.load(open('target_columns.pkl', 'rb'))\n",
        "    for target in targets:\n",
        "        models[target] = pickle.load(open(f'model_{target}.pkl', 'rb'))\n",
        "\n",
        "    recipe_ids = pickle.load(open('recipe_ids.pkl', 'rb'))\n",
        "    nutrition_scaler = pickle.load(open('nutrition_scaler.pkl', 'rb'))\n",
        "\n",
        "    # Load the preprocessed data to get the features\n",
        "    df = pd.read_csv(\"preprocessed_recipes.csv\")\n",
        "\n",
        "    # Extract the features - fill NaN with 0 for prediction stability\n",
        "    features = df.drop(['RecipeId'] + targets, axis=1).fillna(0)\n",
        "\n",
        "    # Get the feature columns from saved list if available\n",
        "    try:\n",
        "        feature_columns = pickle.load(open('feature_columns.pkl', 'rb'))\n",
        "        features = features[feature_columns]\n",
        "    except:\n",
        "        # If feature columns aren't available, use what we have\n",
        "        pass\n",
        "\n",
        "    # Make predictions for each recipe - vectorized approach\n",
        "    predictions = np.zeros((len(recipe_ids), len(targets)))\n",
        "    for i, target in enumerate(targets):\n",
        "        preds = models[target].predict(features)\n",
        "        predictions[:, i] = preds\n",
        "\n",
        "    # Convert predictions back to original scale\n",
        "    predictions_original = nutrition_scaler.inverse_transform(predictions)\n",
        "\n",
        "    # Create a DataFrame with recipe IDs and predictions - optimized creation\n",
        "    results_df = pd.DataFrame({\n",
        "        'RecipeId': recipe_ids,\n",
        "        'Predicted_Protein': predictions_original[:, 0],\n",
        "        'Predicted_Carbs': predictions_original[:, 1],\n",
        "        'Predicted_Fiber': predictions_original[:, 2],\n",
        "        'Predicted_Fat': predictions_original[:, 3]\n",
        "    })\n",
        "\n",
        "    # Make filtering more efficient by:\n",
        "    # 1. Setting wider ranges for the first filter to get enough recipes\n",
        "    # 2. Then sorting by distance and taking the top N\n",
        "\n",
        "    # Initial filtering with wider ranges (±20%)\n",
        "    filtered_recipes = results_df[\n",
        "        (results_df['Predicted_Protein'] >= protein_constraint * 0.8) &\n",
        "        (results_df['Predicted_Protein'] <= protein_constraint * 1.2) &\n",
        "        (results_df['Predicted_Carbs'] >= carbs_constraint * 0.8) &\n",
        "        (results_df['Predicted_Carbs'] <= carbs_constraint * 1.2) &\n",
        "        (results_df['Predicted_Fiber'] >= fiber_constraint * 0.8) &\n",
        "        (results_df['Predicted_Fiber'] <= fiber_constraint * 1.2) &\n",
        "        (results_df['Predicted_Fat'] >= fat_constraint * 0.8) &\n",
        "        (results_df['Predicted_Fat'] <= fat_constraint * 1.2)\n",
        "    ]\n",
        "\n",
        "    # Calculate distance using vectorized operations\n",
        "    filtered_recipes['Distance'] = (\n",
        "        ((filtered_recipes['Predicted_Protein'] - protein_constraint) / protein_constraint) ** 2 +\n",
        "        ((filtered_recipes['Predicted_Carbs'] - carbs_constraint) / carbs_constraint) ** 2 +\n",
        "        ((filtered_recipes['Predicted_Fiber'] - fiber_constraint) / fiber_constraint) ** 2 +\n",
        "        ((filtered_recipes['Predicted_Fat'] - fat_constraint) / fat_constraint) ** 2\n",
        "    )\n",
        "\n",
        "    # Sort by distance\n",
        "    filtered_recipes = filtered_recipes.sort_values('Distance')\n",
        "\n",
        "    # If we still don't have enough recipes, relax constraints further\n",
        "    if len(filtered_recipes) < 5:\n",
        "        filtered_recipes = results_df\n",
        "        filtered_recipes['Distance'] = (\n",
        "            ((filtered_recipes['Predicted_Protein'] - protein_constraint) / max(protein_constraint, 1)) ** 2 +\n",
        "            ((filtered_recipes['Predicted_Carbs'] - carbs_constraint) / max(carbs_constraint, 1)) ** 2 +\n",
        "            ((filtered_recipes['Predicted_Fiber'] - fiber_constraint) / max(fiber_constraint, 1)) ** 2 +\n",
        "            ((filtered_recipes['Predicted_Fat'] - fat_constraint) / max(fat_constraint, 1)) ** 2\n",
        "        )\n",
        "        filtered_recipes = filtered_recipes.sort_values('Distance')\n",
        "\n",
        "    # Get the top N recipes\n",
        "    top_recipes = filtered_recipes.head(limit)\n",
        "\n",
        "    # Load display data to get recipe details\n",
        "    display_data = pickle.load(open('display_data.pkl', 'rb'))\n",
        "\n",
        "    # Merge with display data to get recipe details - optimize the merge\n",
        "    recipe_details = display_data[display_data['RecipeId'].isin(top_recipes['RecipeId'].tolist())]\n",
        "    final_results = pd.merge(recipe_details, top_recipes, on='RecipeId', how='inner')\n",
        "\n",
        "    return final_results\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_recipe_recommendation_model()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "65olp91Amm7Q"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}